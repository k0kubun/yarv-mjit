% trace_enablable_insns = [
%   'opt_send_without_block',
%   'send',
%   'invokeblock',
%   'invokesuper',
% ]
%
% to_cstr = lambda do |line|
%   normalized = line.gsub(/\t/, ' ' * 8)
%   indented = normalized.sub(/\A/, '    ')
%   rstring2cstr(indented.rstrip).sub(/"\z/, '\\n"')
% end
%
    fprintf(f, "{\n");
    {
% # compiler: Prepare operands which may be used by `insn.call_attribute`
% insn.opes.each_with_index do |ope, i|
        MAYBE_UNUSED(<%= ope.fetch(:decl) %>) = (<%= ope.fetch(:type) %>)operands[<%= i %>];
% end
%
% # JIT: Declare variables for operands, popped values and return values
% ret_decls = insn.rets.map { |r| "MAYBE_UNUSED(#{r.fetch(:type)}) #{r.fetch(:name)}"} # TODO: fix #declarations to return Hash...
% insn.declarations.each do |decl|
%   next if dispatched && ret_decls.include?(decl) # return value should be propagated to dispatcher. TODO: assert it's the same as dispatcher
        fprintf(f, "    <%= decl %>;\n");
% end

% # JIT: Set const expressions for `RubyVM::OperandsUnifications` insn
% insn.preamble.each do |amble|
        fprintf(f, "<%= amble.expr %>\n");
% end
%
% # JIT: Initialize operands
% insn.opes.each_with_index do |ope, i|
        fprintf(f, "    <%= ope.fetch(:name) %> = (<%= ope.fetch(:type) %>)0x%"PRIxVALUE";\n", operands[<%= i %>]);
% end
%
% # JIT: Initialize popped values
% insn.pops.reverse_each.with_index.reverse_each do |pop, i|
        fprintf(f, "    <%= pop.fetch(:name) %> = stack[%d];\n", b->stack_size - <%= i + 1 %>);
% end
%
% # JIT: Print insn body in insns.def
% insn.expr.expr.each_line do |line|
%   # Special macro expansion for ones that can't be resolved by macro redefinition.
%   if line =~ /\A\s+ADD_PC\((?<add_pc>[^)]+)\);\s+(\/\*.+\*\/)?\s+\z/ # TODO: create some another macro to avoid this
        operands += <%= Regexp.last_match[:add_pc] %>; /* ADD_PC() !!! */
%   elsif line =~ /\A\s+DISPATCH_ORIGINAL_INSN\((?<insn_name>[^)]+)\);\s+\z/
%     insn_name = Regexp.last_match[:insn_name]
%     dispatch_insn = RubyVM::BareInstructions.fetch(insn_name)

    fprintf(f, "\n/* --- BEGIN: <%= line.strip %> --- */\n");
<%= render 'mjit_compile_insn', locals: { insn: dispatch_insn, dispatched: true } -%>
    fprintf(f, "/* --- END: <%= line.strip %> --- */\n\n");

%   elsif line =~ /\A\s+JUMP\((?<jump>[^)]+)\);\s+\z/
        next_pos = pos + insn_len(insn) + (unsigned int)<%= Regexp.last_match[:jump] %>;
        fprintf(f, "            goto label_%d;\n", next_pos);
%   else
        fprintf(f, <%= to_cstr.call(line) %>);
%   end
% end
%
% # JIT: Set return values
% unless dispatched
%   insn.rets.reverse_each.with_index do |ret, i|
%     # TOPN(n) = ...
        fprintf(f, "    stack[%d] = <%= ret.fetch(:name) %>;\n", b->stack_size + (int)<%= insn.call_attribute('sp_inc') %> - <%= i + 1 %>);
%   end
% end
%
% # JIT: We should evaluate ISeq modified for TracePoint if it's enabled. Note: This is slow.
% if trace_enablable_insns.include?(insn.name)
        fprintf(f, "    if (UNLIKELY(ruby_vm_event_enabled_flags & ISEQ_TRACE_EVENTS)) {\n");
        fprintf(f, "        reg_cfp->sp = reg_cfp->bp + %d;\n", b->stack_size + (int)<%= insn.call_attribute('sp_inc') %> + 1);
        fprintf(f, "        return Qundef; /* cancel JIT */\n");
        fprintf(f, "    }\n");
% end
%
% # compiler: Move JIT compiler's internal stack pointer
% unless dispatched
        b->stack_size += <%= insn.call_attribute('sp_inc') %>;
% end
    }
    fprintf(f, "}\n");
%
% # compiler: If insn has conditional JUMP, the branch which is not targeted by JUMP should be compiled too.
% if insn.expr.expr =~ /if\s+\([^{}]+\)\s+\{[^{}]+JUMP\([^)]+\);[^{}]+\}/
    compile_insns(f, body, b->stack_size, pos + insn_len(insn), status);
% end
